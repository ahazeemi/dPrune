{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# dPrune: Forgetting Score Example\n",
    "\n",
    "This notebook demonstrates how to use the **Forgetting Score** functionality in `dPrune`. The forgetting score is based on the \"Deep Learning on a Data Diet\" paper and measures how many times an example is \"forgotten\" during training.\n",
    "\n",
    "An example is \"forgotten\" if it transitions from being classified correctly to incorrectly between epochs.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install -e .[test]\n",
    "# !pip install transformers datasets torch scikit-learn tqdm accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# dPrune imports\n",
    "from dprune.callbacks import ForgettingCallback\n",
    "from dprune.scorers.supervised import ForgettingScorer\n",
    "from dprune.pruners.selection import TopKPruner, BottomKPruner\n",
    "from dprune.pipeline import PruningPipeline\n",
    "\n",
    "print(\"All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Create a Larger Dataset\n",
    "\n",
    "For the forgetting score to be meaningful, we need a dataset large enough and training long enough to observe forgetting events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more substantial dataset for sentiment analysis\n",
    "positive_texts = [\n",
    "    \"This movie is absolutely fantastic!\",\n",
    "    \"I loved every minute of this film.\",\n",
    "    \"Outstanding performance by all actors.\",\n",
    "    \"A masterpiece of modern cinema.\",\n",
    "    \"Brilliant storytelling and direction.\",\n",
    "    \"This is one of the best movies I've ever seen.\",\n",
    "    \"Incredible cinematography and soundtrack.\",\n",
    "    \"A delightful and heartwarming story.\",\n",
    "    \"Perfect blend of action and emotion.\",\n",
    "    \"This film exceeded all my expectations.\",\n",
    "    \"Amazing special effects and great plot.\",\n",
    "    \"A truly inspiring and uplifting movie.\"\n",
    "]\n",
    "\n",
    "negative_texts = [\n",
    "    \"This movie was a complete waste of time.\",\n",
    "    \"Boring and predictable storyline.\",\n",
    "    \"Poor acting and terrible direction.\",\n",
    "    \"I couldn't wait for this movie to end.\",\n",
    "    \"Disappointing and overrated film.\",\n",
    "    \"The worst movie I've seen this year.\",\n",
    "    \"Confusing plot and bad character development.\",\n",
    "    \"Not worth the money or time.\",\n",
    "    \"Terrible script and poor execution.\",\n",
    "    \"This film was incredibly dull.\",\n",
    "    \"Weak storyline and unconvincing performances.\",\n",
    "    \"A forgettable and mediocre movie.\"\n",
    "]\n",
    "\n",
    "# Combine into dataset\n",
    "texts = positive_texts + negative_texts\n",
    "labels = [1] * len(positive_texts) + [0] * len(negative_texts)  # 1=positive, 0=negative\n",
    "\n",
    "# Shuffle the data\n",
    "indices = list(range(len(texts)))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "shuffled_texts = [texts[i] for i in indices]\n",
    "shuffled_labels = [labels[i] for i in indices]\n",
    "\n",
    "raw_dataset = Dataset.from_dict({\n",
    "    'text': shuffled_texts,\n",
    "    'label': shuffled_labels\n",
    "})\n",
    "\n",
    "print(f\"Dataset created with {len(raw_dataset)} examples\")\n",
    "print(f\"Positive examples: {sum(raw_dataset['label'])}\")\n",
    "print(f\"Negative examples: {len(raw_dataset) - sum(raw_dataset['label'])}\")\n",
    "print(\"\\nFirst few examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {i}: '{raw_dataset['text'][i]}' -> {raw_dataset['label'][i]}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Setup Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "print(f\"Tokenized dataset: {tokenized_dataset}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Initialize the Forgetting Callback\n",
    "\n",
    "This is the key step! We create a `ForgettingCallback` that will monitor the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the forgetting callback\n",
    "forgetting_callback = ForgettingCallback()\n",
    "\n",
    "print(\"ForgettingCallback initialized!\")\n",
    "print(\"This callback will track learning events during training.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Train the Model with the Callback\n",
    "\n",
    "We'll train for several epochs to give the model a chance to \"forget\" some examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments - we want multiple epochs to observe forgetting\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./forgetting_results',\n",
    "    num_train_epochs=5,  # More epochs to observe forgetting\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",  # Don't save checkpoints for this example\n",
    ")\n",
    "\n",
    "# Create trainer with our callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    callbacks=[forgetting_callback],  # This is the key addition!\n",
    ")\n",
    "\n",
    "print(\"Trainer created with ForgettingCallback.\")\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Examine the Forgetting Events\n",
    "\n",
    "Let's look at what the callback recorded during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the callback recorded\n",
    "print(f\"Number of examples tracked: {len(forgetting_callback.learning_events)}\")\n",
    "print(\"\\nLearning events for first 5 examples:\")\n",
    "for i in range(min(5, len(forgetting_callback.learning_events))):\n",
    "    events = forgetting_callback.learning_events.get(i, [])\n",
    "    print(f\"  Example {i}: {events}\")\n",
    "    if events:\n",
    "        # Count transitions from correct (1) to incorrect (0)\n",
    "        transitions = list(zip(events, events[1:]))\n",
    "        forgetting_events = sum(1 for prev, curr in transitions if prev == 1 and curr == 0)\n",
    "        print(f\"    -> Forgetting events: {forgetting_events}\")\n",
    "\n",
    "# Calculate forgetting scores\n",
    "forgetting_scores = forgetting_callback.calculate_forgetting_scores()\n",
    "print(f\"\\nForgetting scores calculated for {len(forgetting_scores)} examples\")\n",
    "print(f\"Score distribution: min={min(forgetting_scores)}, max={max(forgetting_scores)}, mean={np.mean(forgetting_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Use the Forgetting Scorer in a Pipeline\n",
    "\n",
    "Now we can use the populated callback with our `ForgettingScorer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the forgetting scorer using our callback\n",
    "forgetting_scorer = ForgettingScorer(forgetting_callback)\n",
    "\n",
    "# Score the dataset\n",
    "scored_dataset = forgetting_scorer.score(raw_dataset)\n",
    "\n",
    "print(\"Dataset scored with forgetting scores!\")\n",
    "print(f\"Scored dataset columns: {scored_dataset.column_names}\")\n",
    "print(\"\\nFirst few examples with scores:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Score: {scored_dataset['score'][i]}, Text: '{scored_dataset['text'][i][:50]}...', Label: {scored_dataset['label'][i]}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Pruning with Forgetting Scores\n",
    "\n",
    "Let's compare different pruning strategies using the forgetting scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Keep examples that are forgotten the most (hardest examples)\n",
    "top_pruner = TopKPruner(k=0.5)  # Keep top 50%\n",
    "pipeline_hard = PruningPipeline(scorer=forgetting_scorer, pruner=top_pruner)\n",
    "hard_examples = pipeline_hard.run(raw_dataset)\n",
    "\n",
    "# Strategy 2: Keep examples that are never forgotten (easy/stable examples)\n",
    "bottom_pruner = BottomKPruner(k=0.5)  # Keep bottom 50%\n",
    "pipeline_easy = PruningPipeline(scorer=forgetting_scorer, pruner=bottom_pruner)\n",
    "easy_examples = pipeline_easy.run(raw_dataset)\n",
    "\n",
    "print(\"Pruning Results:\")\n",
    "print(f\"Original dataset: {len(raw_dataset)} examples\")\n",
    "print(f\"Hard examples (most forgotten): {len(hard_examples)} examples\")\n",
    "print(f\"Easy examples (least forgotten): {len(easy_examples)} examples\")\n",
    "\n",
    "print(\"\\nHardest examples (most forgotten):\")\n",
    "for i in range(min(3, len(hard_examples))):\n",
    "    print(f\"  Score: {hard_examples['score'][i]}, Text: '{hard_examples['text'][i][:60]}...', Label: {hard_examples['label'][i]}\")\n",
    "\n",
    "print(\"\\nEasiest examples (least forgotten):\")\n",
    "for i in range(min(3, len(easy_examples))):\n",
    "    print(f\"  Score: {easy_examples['score'][i]}, Text: '{easy_examples['text'][i][:60]}...', Label: {easy_examples['label'][i]}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9. Analysis and Insights\n",
    "\n",
    "Let's analyze the relationship between forgetting scores and the examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot distribution of forgetting scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scored_dataset['score'], bins=max(1, max(scored_dataset['score']) + 1), alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Forgetting Score')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Distribution of Forgetting Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Analyze by label\n",
    "positive_scores = [score for score, label in zip(scored_dataset['score'], scored_dataset['label']) if label == 1]\n",
    "negative_scores = [score for score, label in zip(scored_dataset['score'], scored_dataset['label']) if label == 0]\n",
    "\n",
    "print(f\"\\nForgetting Score Analysis:\")\n",
    "print(f\"Positive examples - Mean: {np.mean(positive_scores):.2f}, Std: {np.std(positive_scores):.2f}\")\n",
    "print(f\"Negative examples - Mean: {np.mean(negative_scores):.2f}, Std: {np.std(negative_scores):.2f}\")\n",
    "\n",
    "# Find the most and least forgotten examples\n",
    "max_score = max(scored_dataset['score'])\n",
    "min_score = min(scored_dataset['score'])\n",
    "\n",
    "most_forgotten_idx = scored_dataset['score'].index(max_score)\n",
    "least_forgotten_idx = scored_dataset['score'].index(min_score)\n",
    "\n",
    "print(f\"\\nMost forgotten example (score: {max_score}):\")\n",
    "print(f\"  '{scored_dataset['text'][most_forgotten_idx]}' (Label: {scored_dataset['label'][most_forgotten_idx]})\")\n",
    "\n",
    "print(f\"\\nLeast forgotten example (score: {min_score}):\")\n",
    "print(f\"  '{scored_dataset['text'][least_forgotten_idx]}' (Label: {scored_dataset['label'][least_forgotten_idx]})\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Set up the `ForgettingCallback`** to monitor training\n",
    "2. **Train a model** while recording learning events\n",
    "3. **Calculate forgetting scores** from the recorded events\n",
    "4. **Use the `ForgettingScorer`** in a `dPrune` pipeline\n",
    "5. **Compare different pruning strategies** based on forgetting scores\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **High forgetting scores** indicate examples that the model struggles with consistently\n",
    "- **Low forgetting scores** indicate examples that are learned early and retained\n",
    "- **Pruning strategies** can focus on either hard examples (for challenging training) or easy examples (for stable training)\n",
    "\n",
    "The forgetting score is a powerful metric for understanding model learning dynamics and can guide intelligent data selection strategies!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
