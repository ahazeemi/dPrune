{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Unsupervised Data Pruning with Perplexity Scoring for Summarization\n",
        "\n",
        "This notebook demonstrates how to use the `PerplexityScorer` to improve dataset quality for text summarization tasks using the CNN/DailyMail dataset. We'll train summarization models on both original and pruned datasets and compare their ROUGE-L scores.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The `PerplexityScorer` calculates perplexity scores for text using a KenLM language model. Higher perplexity indicates harder (and potentially more informative) instances, while lower perplexity indicates easier and more prototypical instances.\n",
        "\n",
        "For summarization tasks, we can use perplexity scoring to:\n",
        "- Remove extremely low-quality or malformed articles\n",
        "- Filter out articles that are too difficult or unusual\n",
        "- Keep a balanced dataset of informative yet learnable examples\n",
        "\n",
        "## Requirements\n",
        "\n",
        "You'll need to install the dependencies:\n",
        "\n",
        "```bash\n",
        "pip install dprune transformers rouge-score nltk\n",
        "```\n",
        "\n",
        "You'll also need a KenLM language model. For this example, we'll download a pre-trained English model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM, \n",
        "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq, pipeline\n",
        ")\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "from dprune.scorers import PerplexityScorer\n",
        "from dprune.pruners import TopKPruner, BottomKPruner\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Download KenLM Model and Load Dataset\n",
        "\n",
        "First, we'll download a pre-trained KenLM model and load the CNN/DailyMail dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and setup KenLM model (optional - for demo we'll use mock scores)\n",
        "# You can download a pre-trained KenLM model from:\n",
        "# https://github.com/kpu/kenlm or https://huggingface.co/models?search=kenlm\n",
        "\n",
        "# For this demo, we'll show how to use PerplexityScorer with a real model:\n",
        "# KENLM_MODEL_PATH = \"/path/to/your/kenlm/model.bin\"\n",
        "\n",
        "# Load CNN/DailyMail dataset\n",
        "print(\"Loading CNN/DailyMail dataset...\")\n",
        "dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
        "\n",
        "# Take a subset for faster experimentation (remove this for full dataset)\n",
        "SUBSET_SIZE = 1000  # Adjust based on your computational resources\n",
        "dataset = dataset.select(range(SUBSET_SIZE))\n",
        "\n",
        "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
        "print(f\"Dataset columns: {dataset.column_names}\")\n",
        "\n",
        "# Show a sample\n",
        "sample = dataset[0]\n",
        "print(f\"\\nSample article (first 300 chars): {sample['article'][:300]}...\")\n",
        "print(f\"Sample highlights (first 200 chars): {sample['highlights'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Calculate Perplexity Scores\n",
        "\n",
        "We'll use the PerplexityScorer to score the articles based on their text quality and complexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate perplexity scores\n",
        "# Method 1: Using a real KenLM model (uncomment if you have one)\n",
        "# scorer = PerplexityScorer(\n",
        "#     model_path=KENLM_MODEL_PATH,\n",
        "#     text_column='article',\n",
        "#     batch_size=50\n",
        "# )\n",
        "# scored_dataset = scorer.score(dataset)\n",
        "\n",
        "# Method 2: Generate realistic mock scores for demonstration\n",
        "def generate_mock_perplexity_scores(articles):\n",
        "    \"\"\"Generate mock perplexity scores based on article characteristics.\"\"\"\n",
        "    scores = []\n",
        "    for article in tqdm(articles, desc=\"Generating mock perplexity scores\"):\n",
        "        # Calculate features that correlate with perplexity\n",
        "        word_count = len(article.split())\n",
        "        sentence_count = len([s for s in article.split('.') if s.strip()])\n",
        "        avg_sentence_length = word_count / max(sentence_count, 1)\n",
        "        \n",
        "        # Count complex words (longer than 6 characters)\n",
        "        complex_words = len([w for w in article.split() if len(w) > 6])\n",
        "        complexity_ratio = complex_words / max(word_count, 1)\n",
        "        \n",
        "        # Base perplexity on text characteristics\n",
        "        # Longer articles with more complex words tend to have higher perplexity\n",
        "        base_score = 50 + (word_count * 0.05) + (complexity_ratio * 100)\n",
        "        \n",
        "        # Add some noise to make it more realistic\n",
        "        noise = np.random.normal(0, 10)\n",
        "        score = max(10, base_score + noise)  # Ensure minimum score of 10\n",
        "        scores.append(score)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "# Generate mock scores\n",
        "mock_scores = generate_mock_perplexity_scores(dataset['article'])\n",
        "scored_dataset = dataset.add_column('perplexity_score', mock_scores)\n",
        "\n",
        "print(f\"Perplexity scores calculated for {len(scored_dataset)} articles\")\n",
        "print(f\"Score statistics:\")\n",
        "print(f\"  Mean: {np.mean(mock_scores):.2f}\")\n",
        "print(f\"  Median: {np.median(mock_scores):.2f}\")\n",
        "print(f\"  Min: {np.min(mock_scores):.2f}\")\n",
        "print(f\"  Max: {np.max(mock_scores):.2f}\")\n",
        "print(f\"  Std: {np.std(mock_scores):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize perplexity distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(mock_scores, bins=30, alpha=0.7, edgecolor='black')\n",
        "plt.axvline(np.mean(mock_scores), color='red', linestyle='--', label=f'Mean: {np.mean(mock_scores):.1f}')\n",
        "plt.axvline(np.median(mock_scores), color='green', linestyle='--', label=f'Median: {np.median(mock_scores):.1f}')\n",
        "plt.xlabel('Perplexity Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Perplexity Scores')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "article_lengths = [len(article.split()) for article in dataset['article']]\n",
        "plt.scatter(article_lengths, mock_scores, alpha=0.6, s=20)\n",
        "plt.xlabel('Article Length (words)')\n",
        "plt.ylabel('Perplexity Score')\n",
        "plt.title('Perplexity vs Article Length')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show examples of high and low perplexity articles\n",
        "sorted_indices = np.argsort(mock_scores)\n",
        "\n",
        "print(\"\\\\n=== LOWEST PERPLEXITY ARTICLES (Easiest/Most Prototypical) ===\")\n",
        "for i in range(3):\n",
        "    idx = sorted_indices[i]\n",
        "    print(f\"\\\\nScore: {mock_scores[idx]:.1f}\")\n",
        "    print(f\"Article: {scored_dataset[idx]['article'][:200]}...\")\n",
        "    print(f\"Summary: {scored_dataset[idx]['highlights'][:100]}...\")\n",
        "\n",
        "print(\"\\\\n=== HIGHEST PERPLEXITY ARTICLES (Hardest/Most Complex) ===\")\n",
        "for i in range(3):\n",
        "    idx = sorted_indices[-(i+1)]\n",
        "    print(f\"\\\\nScore: {mock_scores[idx]:.1f}\")\n",
        "    print(f\"Article: {scored_dataset[idx]['article'][:200]}...\")\n",
        "    print(f\"Summary: {scored_dataset[idx]['highlights'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Prune the Dataset\n",
        "\n",
        "We'll create pruned versions of the dataset by removing extreme outliers (both very high and very low perplexity) to focus on a balanced set of informative examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create different pruned datasets for comparison\n",
        "# We'll compare:\n",
        "# 1. Original dataset (no pruning)\n",
        "# 2. Pruned dataset (remove extreme outliers - top and bottom 20%)\n",
        "\n",
        "# For proper pruning with perplexity scores, we want to remove:\n",
        "# - Articles with extremely low perplexity (too simple/repetitive)\n",
        "# - Articles with extremely high perplexity (too complex/noisy)\n",
        "\n",
        "# Calculate pruning thresholds\n",
        "perplexity_scores = np.array(mock_scores)\n",
        "low_threshold = np.percentile(perplexity_scores, 20)  # Bottom 20%\n",
        "high_threshold = np.percentile(perplexity_scores, 80)  # Top 20%\n",
        "\n",
        "print(f\"Perplexity thresholds:\")\n",
        "print(f\"  Low threshold (20th percentile): {low_threshold:.2f}\")\n",
        "print(f\"  High threshold (80th percentile): {high_threshold:.2f}\")\n",
        "\n",
        "# Create masks for filtering\n",
        "middle_range_mask = (perplexity_scores >= low_threshold) & (perplexity_scores <= high_threshold)\n",
        "\n",
        "# Create datasets\n",
        "original_dataset = scored_dataset\n",
        "pruned_dataset = scored_dataset.filter(lambda example, idx: middle_range_mask[idx], with_indices=True)\n",
        "\n",
        "print(f\"\\\\nDataset sizes:\")\n",
        "print(f\"  Original: {len(original_dataset)} examples\")\n",
        "print(f\"  Pruned (middle 60%): {len(pruned_dataset)} examples\")\n",
        "print(f\"  Reduction: {len(original_dataset) - len(pruned_dataset)} examples ({(1 - len(pruned_dataset)/len(original_dataset))*100:.1f}%)\")\n",
        "\n",
        "# Compare perplexity distributions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(original_dataset['perplexity_score'], bins=30, alpha=0.6, label='Original Dataset', color='blue')\n",
        "plt.hist(pruned_dataset['perplexity_score'], bins=30, alpha=0.6, label='Pruned Dataset', color='red')\n",
        "plt.axvline(low_threshold, color='green', linestyle='--', label=f'Low threshold: {low_threshold:.1f}')\n",
        "plt.axvline(high_threshold, color='orange', linestyle='--', label=f'High threshold: {high_threshold:.1f}')\n",
        "plt.xlabel('Perplexity Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Perplexity Distribution: Original vs Pruned Dataset')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Show statistics for both datasets\n",
        "def dataset_stats(dataset, name):\n",
        "    scores = dataset['perplexity_score']\n",
        "    lengths = [len(article.split()) for article in dataset['article']]\n",
        "    \n",
        "    print(f\"\\\\n{name} Dataset Statistics:\")\n",
        "    print(f\"  Size: {len(dataset)} examples\")\n",
        "    print(f\"  Perplexity - Mean: {np.mean(scores):.2f}, Std: {np.std(scores):.2f}\")\n",
        "    print(f\"  Article length - Mean: {np.mean(lengths):.1f} words, Std: {np.std(lengths):.1f}\")\n",
        "\n",
        "dataset_stats(original_dataset, \"Original\")\n",
        "dataset_stats(pruned_dataset, \"Pruned\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: Train Summarization Models\n",
        "\n",
        "We'll train lightweight summarization models on both the original and pruned datasets, then compare their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"facebook/bart-large-cnn\"  # Pre-trained summarization model\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 5e-5\n",
        "NUM_EPOCHS = 2  # Small number for demo - increase for better results\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"Loaded tokenizer for {MODEL_NAME}\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Preprocess the data for summarization.\"\"\"\n",
        "    # Prepare inputs and targets\n",
        "    inputs = [f\"summarize: {article}\" for article in examples[\"article\"]]\n",
        "    targets = examples[\"highlights\"]\n",
        "    \n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, \n",
        "        max_length=MAX_INPUT_LENGTH, \n",
        "        truncation=True, \n",
        "        padding=True\n",
        "    )\n",
        "    \n",
        "    # Tokenize targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets, \n",
        "            max_length=MAX_TARGET_LENGTH, \n",
        "            truncation=True, \n",
        "            padding=True\n",
        "        )\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "print(\"Preprocessing datasets...\")\n",
        "\n",
        "# Preprocess both datasets\n",
        "tokenized_original = original_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_pruned = pruned_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Split datasets into train/test (80/20 split)\n",
        "def split_dataset(dataset, test_size=0.2):\n",
        "    dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
        "    return dataset[\"train\"], dataset[\"test\"]\n",
        "\n",
        "original_train, original_test = split_dataset(tokenized_original)\n",
        "pruned_train, pruned_test = split_dataset(tokenized_pruned)\n",
        "\n",
        "print(f\"\\nDataset splits:\")\n",
        "print(f\"Original - Train: {len(original_train)}, Test: {len(original_test)}\")\n",
        "print(f\"Pruned - Train: {len(pruned_train)}, Test: {len(pruned_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(train_dataset, output_dir, model_name=\"Model\"):\n",
        "    \"\"\"Train a summarization model.\"\"\"\n",
        "    print(f\"\\n=== Training {model_name} ===\")\n",
        "    \n",
        "    # Load fresh model for each training\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"no\",  # Disable evaluation during training for speed\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        weight_decay=0.01,\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        predict_with_generate=True,\n",
        "        fp16=torch.cuda.is_available(),  # Use mixed precision if CUDA available\n",
        "        dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
        "        logging_steps=50,\n",
        "        report_to=None,  # Disable wandb logging\n",
        "    )\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "    \n",
        "    # Initialize trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    print(f\"Starting training with {len(train_dataset)} examples...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    # Save the model\n",
        "    trainer.save_model()\n",
        "    print(f\"Model saved to {output_dir}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train both models\n",
        "print(\"Starting model training...\")\n",
        "print(\"Note: This may take some time depending on your hardware.\")\n",
        "\n",
        "# Train on original dataset\n",
        "original_model = train_model(\n",
        "    train_dataset=original_train,\n",
        "    output_dir=\"./model_original\",\n",
        "    model_name=\"Original Dataset Model\"\n",
        ")\n",
        "\n",
        "# Train on pruned dataset  \n",
        "pruned_model = train_model(\n",
        "    train_dataset=pruned_train,\n",
        "    output_dir=\"./model_pruned\", \n",
        "    model_name=\"Pruned Dataset Model\"\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Both models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 5: Evaluate Models with ROUGE-L Scores\n",
        "\n",
        "Now we'll evaluate both models on test sets and compare their ROUGE-L scores to see the impact of perplexity-based pruning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ROUGE scorer\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "def generate_summaries(model, test_dataset, max_samples=None):\n",
        "    \"\"\"Generate summaries using the trained model.\"\"\"\n",
        "    if max_samples:\n",
        "        test_dataset = test_dataset.select(range(min(max_samples, len(test_dataset))))\n",
        "    \n",
        "    summaries = []\n",
        "    references = []\n",
        "    \n",
        "    # Create a pipeline for easier inference\n",
        "    summarizer = pipeline(\n",
        "        \"summarization\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=MAX_TARGET_LENGTH,\n",
        "        min_length=30,\n",
        "        do_sample=False,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    \n",
        "    print(f\"Generating summaries for {len(test_dataset)} examples...\")\n",
        "    \n",
        "    for i, example in enumerate(tqdm(test_dataset)):\n",
        "        # Get original article and reference summary\n",
        "        article = example['article']\n",
        "        reference = example['highlights']\n",
        "        \n",
        "        try:\n",
        "            # Generate summary\n",
        "            result = summarizer(article, max_length=MAX_TARGET_LENGTH, min_length=30, do_sample=False)\n",
        "            generated_summary = result[0]['summary_text']\n",
        "            \n",
        "            summaries.append(generated_summary)\n",
        "            references.append(reference)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing example {i}: {e}\")\n",
        "            summaries.append(\"\")  # Empty summary for failed cases\n",
        "            references.append(reference)\n",
        "    \n",
        "    return summaries, references\n",
        "\n",
        "def calculate_rouge_scores(summaries, references):\n",
        "    \"\"\"Calculate ROUGE-L scores.\"\"\"\n",
        "    rouge_l_scores = []\n",
        "    \n",
        "    for summary, reference in zip(summaries, references):\n",
        "        if summary.strip():  # Only calculate if summary is not empty\n",
        "            scores = rouge_scorer_instance.score(reference, summary)\n",
        "            rouge_l_scores.append(scores['rougeL'].fmeasure)\n",
        "        else:\n",
        "            rouge_l_scores.append(0.0)  # Score 0 for empty summaries\n",
        "    \n",
        "    return rouge_l_scores\n",
        "\n",
        "# Evaluate both models\n",
        "print(\"\\n=== EVALUATING MODELS ===\")\n",
        "\n",
        "# For faster evaluation, limit test samples (remove this for full evaluation)\n",
        "MAX_TEST_SAMPLES = 50\n",
        "\n",
        "# Evaluate original model\n",
        "print(\"\\nEvaluating Original Dataset Model...\")\n",
        "original_summaries, original_references = generate_summaries(\n",
        "    original_model, original_test, max_samples=MAX_TEST_SAMPLES\n",
        ")\n",
        "original_rouge_scores = calculate_rouge_scores(original_summaries, original_references)\n",
        "\n",
        "# Evaluate pruned model\n",
        "print(\"\\nEvaluating Pruned Dataset Model...\")\n",
        "pruned_summaries, pruned_references = generate_summaries(\n",
        "    pruned_model, pruned_test, max_samples=MAX_TEST_SAMPLES\n",
        ")\n",
        "pruned_rouge_scores = calculate_rouge_scores(pruned_summaries, pruned_references)\n",
        "\n",
        "# Calculate statistics\n",
        "original_mean_rouge = np.mean(original_rouge_scores)\n",
        "original_std_rouge = np.std(original_rouge_scores)\n",
        "pruned_mean_rouge = np.mean(pruned_rouge_scores)\n",
        "pruned_std_rouge = np.std(pruned_rouge_scores)\n",
        "\n",
        "print(f\"\\n=== ROUGE-L RESULTS ===\")\n",
        "print(f\"Original Dataset Model:\")\n",
        "print(f\"  Mean ROUGE-L: {original_mean_rouge:.4f} (±{original_std_rouge:.4f})\")\n",
        "print(f\"  Test samples: {len(original_rouge_scores)}\")\n",
        "\n",
        "print(f\"\\nPruned Dataset Model:\")\n",
        "print(f\"  Mean ROUGE-L: {pruned_mean_rouge:.4f} (±{pruned_std_rouge:.4f})\")\n",
        "print(f\"  Test samples: {len(pruned_rouge_scores)}\")\n",
        "\n",
        "print(f\"\\nImprovement:\")\n",
        "improvement = pruned_mean_rouge - original_mean_rouge\n",
        "improvement_pct = (improvement / original_mean_rouge) * 100\n",
        "print(f\"  Absolute: {improvement:+.4f}\")\n",
        "print(f\"  Relative: {improvement_pct:+.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: ROUGE-L score distributions\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(original_rouge_scores, bins=20, alpha=0.6, label='Original Model', color='blue')\n",
        "plt.hist(pruned_rouge_scores, bins=20, alpha=0.6, label='Pruned Model', color='red')\n",
        "plt.axvline(original_mean_rouge, color='blue', linestyle='--', \n",
        "           label=f'Original Mean: {original_mean_rouge:.3f}')\n",
        "plt.axvline(pruned_mean_rouge, color='red', linestyle='--', \n",
        "           label=f'Pruned Mean: {pruned_mean_rouge:.3f}')\n",
        "plt.xlabel('ROUGE-L Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('ROUGE-L Score Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Box plot comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "data_to_plot = [original_rouge_scores, pruned_rouge_scores]\n",
        "labels = ['Original\\nModel', 'Pruned\\nModel']\n",
        "bp = plt.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
        "bp['boxes'][0].set_facecolor('lightblue')\n",
        "bp['boxes'][1].set_facecolor('lightcoral')\n",
        "plt.ylabel('ROUGE-L Score')\n",
        "plt.title('ROUGE-L Score Comparison')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show some example summaries\n",
        "print(\"\\n=== EXAMPLE SUMMARIES ===\")\n",
        "\n",
        "for i in range(min(3, len(original_summaries))):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Article (first 200 chars): {original_test[i]['article'][:200]}...\")\n",
        "    print(f\"\\nReference Summary: {original_references[i]}\")\n",
        "    print(f\"\\nOriginal Model Summary: {original_summaries[i]}\")\n",
        "    print(f\"Pruned Model Summary: {pruned_summaries[i]}\")\n",
        "    print(f\"\\nROUGE-L Scores:\")\n",
        "    print(f\"  Original Model: {original_rouge_scores[i]:.4f}\")\n",
        "    print(f\"  Pruned Model: {pruned_rouge_scores[i]:.4f}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 6: Analysis and Conclusions\n",
        "\n",
        "Let's analyze the results and understand the impact of perplexity-based pruning on summarization model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical significance test (t-test)\n",
        "from scipy import stats\n",
        "\n",
        "t_stat, p_value = stats.ttest_ind(pruned_rouge_scores, original_rouge_scores)\n",
        "\n",
        "print(\"=== DETAILED ANALYSIS ===\")\n",
        "print(f\"\\n📊 Statistical Analysis:\")\n",
        "print(f\"  t-statistic: {t_stat:.4f}\")\n",
        "print(f\"  p-value: {p_value:.4f}\")\n",
        "print(f\"  Significant improvement: {'Yes' if p_value < 0.05 else 'No'} (α = 0.05)\")\n",
        "\n",
        "# Training efficiency analysis\n",
        "print(f\"\\n⚡ Training Efficiency:\")\n",
        "print(f\"  Original dataset size: {len(original_train)} examples\")\n",
        "print(f\"  Pruned dataset size: {len(pruned_train)} examples\")\n",
        "print(f\"  Training data reduction: {(1 - len(pruned_train)/len(original_train))*100:.1f}%\")\n",
        "print(f\"  Performance change: {improvement_pct:+.2f}%\")\n",
        "\n",
        "efficiency_ratio = improvement_pct / ((len(original_train) - len(pruned_train))/len(original_train)*100)\n",
        "print(f\"  Efficiency ratio: {efficiency_ratio:.2f} (performance gain per % data reduction)\")\n",
        "\n",
        "print(f\"\\n🎯 Key Findings:\")\n",
        "if improvement > 0:\n",
        "    print(f\"  ✅ Perplexity-based pruning IMPROVED model performance\")\n",
        "    print(f\"  ✅ Achieved {improvement_pct:+.2f}% relative improvement in ROUGE-L\")\n",
        "    print(f\"  ✅ Used {(1 - len(pruned_train)/len(original_train))*100:.1f}% less training data\")\n",
        "else:\n",
        "    print(f\"  ⚠️  Perplexity-based pruning showed {improvement_pct:.2f}% change in ROUGE-L\")\n",
        "    print(f\"  ℹ️  Results may vary with different datasets and model configurations\")\n",
        "\n",
        "print(f\"\\n💡 Insights:\")\n",
        "print(f\"  • Removing extreme perplexity outliers can improve model training\")\n",
        "print(f\"  • Lower perplexity articles may be too simple/repetitive\")\n",
        "print(f\"  • Higher perplexity articles may be too noisy/complex\")\n",
        "print(f\"  • The 'sweet spot' (middle perplexity range) contains optimal training examples\")\n",
        "\n",
        "print(f\"\\n🔧 Recommendations for Production:\")\n",
        "print(f\"  1. Use real KenLM models for more accurate perplexity scoring\")\n",
        "print(f\"  2. Experiment with different pruning thresholds (e.g., remove top/bottom 10-30%)\")\n",
        "print(f\"  3. Consider domain-specific KenLM models for better scoring\")\n",
        "print(f\"  4. Validate results on larger datasets and longer training runs\")\n",
        "print(f\"  5. Combine perplexity pruning with other data quality metrics\")\n",
        "\n",
        "# Save results summary\n",
        "results_summary = {\n",
        "    'original_rouge_mean': original_mean_rouge,\n",
        "    'original_rouge_std': original_std_rouge,\n",
        "    'pruned_rouge_mean': pruned_mean_rouge,\n",
        "    'pruned_rouge_std': pruned_std_rouge,\n",
        "    'improvement_absolute': improvement,\n",
        "    'improvement_relative_pct': improvement_pct,\n",
        "    'original_dataset_size': len(original_train),\n",
        "    'pruned_dataset_size': len(pruned_train),\n",
        "    'data_reduction_pct': (1 - len(pruned_train)/len(original_train))*100,\n",
        "    't_statistic': t_stat,\n",
        "    'p_value': p_value\n",
        "}\n",
        "\n",
        "print(f\"\\n💾 Results saved to 'results_summary' dictionary for further analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated how to use **PerplexityScorer** from dPrune to improve text summarization models through intelligent data pruning. \n",
        "\n",
        "### What We Accomplished:\n",
        "\n",
        "1. **Loaded CNN/DailyMail dataset** - A real-world summarization dataset\n",
        "2. **Calculated perplexity scores** - Used mock scores based on text characteristics (in production, use real KenLM models)\n",
        "3. **Applied intelligent pruning** - Removed extreme perplexity outliers (top and bottom 20%)\n",
        "4. **Trained two models** - One on original data, one on pruned data\n",
        "5. **Compared performance** - Used ROUGE-L scores to measure summarization quality\n",
        "6. **Analyzed results** - Statistical analysis of the performance differences\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- **Perplexity-based pruning can improve model performance** while reducing training data\n",
        "- **Quality over quantity**: A smaller, well-curated dataset often outperforms a larger, noisy one\n",
        "- **Sweet spot principle**: Medium perplexity examples provide the best balance of informativeness and learnability\n",
        "- **Efficiency gains**: Better performance with less data means faster training and lower costs\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "For production use, consider:\n",
        "- Using real KenLM models for accurate perplexity calculation\n",
        "- Experimenting with different pruning thresholds\n",
        "- Validating on larger datasets and longer training runs\n",
        "- Combining perplexity scores with other data quality metrics\n",
        "- Domain-specific language models for specialized applications\n",
        "\n",
        "The PerplexityScorer is a powerful tool for improving dataset quality and model performance in NLP tasks!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
