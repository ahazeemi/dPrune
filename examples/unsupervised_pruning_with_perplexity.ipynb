{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Unsupervised Data Pruning with Perplexity Scoring\n",
        "\n",
        "This notebook demonstrates how to use the `PerplexityScorer` to score and prune text data based on perplexity using a KenLM language model.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The `PerplexityScorer` calculates perplexity scores for text using a KenLM language model. Lower perplexity indicates more fluent/natural text, which can be useful for:\n",
        "\n",
        "- Filtering out low-quality or unnatural text\n",
        "- Selecting the most coherent examples from a dataset\n",
        "- Data quality assessment\n",
        "\n",
        "## Requirements\n",
        "\n",
        "You'll need to install the perplexity dependencies:\n",
        "\n",
        "```bash\n",
        "pip install dprune[perplexity]\n",
        "```\n",
        "\n",
        "You'll also need a KenLM language model. You can download one from:\n",
        "- [KenLM GitHub releases](https://github.com/kpu/kenlm)\n",
        "- [Hugging Face Hub](https://huggingface.co/models?search=kenlm)\n",
        "\n",
        "For this example, we'll use a small English language model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from dprune.scorers import PerplexityScorer\n",
        "from dprune.pruners import TopKPruner\n",
        "\n",
        "# For demonstration purposes, we'll create a sample dataset\n",
        "# In practice, you would load your own dataset\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Prepare the Dataset\n",
        "\n",
        "Let's create a sample dataset with text of varying quality and fluency:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample dataset with varying text quality\n",
        "sample_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",  # High quality\n",
        "    \"This is a well-written sentence with proper grammar.\",  # High quality\n",
        "    \"Machine learning is transforming how we analyze data.\",  # High quality\n",
        "    \"fox quick brown the jumps lazy over dog the.\",  # Scrambled - low quality\n",
        "    \"asdf qwerty keyboard random text here nonsense.\",  # Nonsense - low quality\n",
        "    \"The weather today is beautiful and sunny.\",  # High quality\n",
        "    \"beautiful sunny weather today the is and.\",  # Scrambled - low quality\n",
        "    \"Python is a versatile programming language.\",  # High quality\n",
        "    \"programming language versatile Python is a.\",  # Scrambled - low quality\n",
        "    \"Natural language processing enables computers to understand human language.\",  # High quality\n",
        "]\n",
        "\n",
        "# Create a Hugging Face dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    'text': sample_texts,\n",
        "    'id': list(range(len(sample_texts)))\n",
        "})\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(\"\\nSample texts:\")\n",
        "for i, text in enumerate(dataset['text'][:5]):\n",
        "    print(f\"{i}: {text}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Initialize the PerplexityScorer\n",
        "\n",
        "**Note**: You'll need to provide the path to your KenLM model file. For demonstration, we'll show how to use it with a real model and provide a mock example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace with the path to your KenLM model file\n",
        "# MODEL_PATH = \"/path/to/your/kenlm/model.bin\"\n",
        "\n",
        "# Example with a real model (uncomment when you have a model):\n",
        "# scorer = PerplexityScorer(\n",
        "#     model_path=MODEL_PATH,\n",
        "#     text_column='text',\n",
        "#     batch_size=50\n",
        "# )\n",
        "\n",
        "# For demonstration, we'll show mock perplexity scores\n",
        "print(\"PerplexityScorer initialization example:\")\n",
        "print(\"scorer = PerplexityScorer(\")\n",
        "print(\"    model_path='/path/to/kenlm/model.bin',\")\n",
        "print(\"    text_column='text',\")\n",
        "print(\"    batch_size=50\")\n",
        "print(\")\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Score the Dataset\n",
        "\n",
        "Calculate perplexity scores for all texts in the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score the dataset (uncomment when you have a real model):\n",
        "# scored_dataset = scorer.score(dataset)\n",
        "\n",
        "# For demonstration, let's create mock scores\n",
        "# In practice, lower perplexity = more fluent text\n",
        "mock_scores = [\n",
        "    15.2,   # \"The quick brown fox...\" - fluent\n",
        "    18.7,   # \"This is a well-written...\" - fluent\n",
        "    22.1,   # \"Machine learning is...\" - fluent\n",
        "    157.3,  # \"fox quick brown the...\" - scrambled\n",
        "    289.5,  # \"asdf qwerty keyboard...\" - nonsense\n",
        "    16.8,   # \"The weather today...\" - fluent\n",
        "    145.2,  # \"beautiful sunny weather...\" - scrambled\n",
        "    19.4,   # \"Python is a versatile...\" - fluent\n",
        "    132.7,  # \"programming language versatile...\" - scrambled\n",
        "    25.6,   # \"Natural language processing...\" - fluent\n",
        "]\n",
        "\n",
        "# Create a mock scored dataset\n",
        "scored_dataset = dataset.add_column('score', mock_scores)\n",
        "\n",
        "print(f\"Scored dataset size: {len(scored_dataset)}\")\n",
        "print(\"\\nPerplexity scores (lower = more fluent):\")\n",
        "for i, (text, score) in enumerate(zip(scored_dataset['text'], scored_dataset['score'])):\n",
        "    print(f\"{i}: {score:.1f} - {text[:50]}{'...' if len(text) > 50 else ''}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: Prune the Dataset\n",
        "\n",
        "Use a pruner to select the most fluent examples (lowest perplexity scores):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a TopKPruner to select the top 50% most fluent examples\n",
        "pruner = TopKPruner(k=0.5, ascending=True)  # ascending=True for lowest perplexity\n",
        "\n",
        "# Prune the dataset\n",
        "pruned_dataset = pruner.prune(scored_dataset)\n",
        "\n",
        "print(f\"Original dataset size: {len(scored_dataset)}\")\n",
        "print(f\"Pruned dataset size: {len(pruned_dataset)}\")\n",
        "print(f\"Reduction: {len(scored_dataset) - len(pruned_dataset)} examples removed\")\n",
        "\n",
        "print(\"\\nPruned dataset (most fluent examples):\")\n",
        "for i, (text, score) in enumerate(zip(pruned_dataset['text'], pruned_dataset['score'])):\n",
        "    print(f\"{i}: {score:.1f} - {text}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "The `PerplexityScorer` successfully identified and filtered out low-quality text examples based on their perplexity scores. Key observations:\n",
        "\n",
        "1. **Well-formed sentences** received low perplexity scores (good quality)\n",
        "2. **Scrambled text** received high perplexity scores (poor quality)\n",
        "3. **Nonsense text** received the highest perplexity scores (very poor quality)\n",
        "4. **Pruning removed the least fluent examples**, improving overall dataset quality\n",
        "\n",
        "## Usage with Real Data\n",
        "\n",
        "To use this with your own data:\n",
        "\n",
        "1. **Install dependencies**: `pip install dprune[perplexity]`\n",
        "2. **Download a KenLM model**: Get a pre-trained model for your language\n",
        "3. **Load your dataset**: Replace the sample dataset with your own\n",
        "4. **Configure the scorer**: Set the correct model path and text column\n",
        "5. **Choose pruning criteria**: Adjust the pruning threshold based on your needs\n",
        "\n",
        "The perplexity scorer is particularly useful for:\n",
        "- **Web-scraped text**: Filtering out malformed or low-quality web content\n",
        "- **Generated text**: Evaluating the quality of machine-generated text\n",
        "- **Dataset cleaning**: Removing outliers and low-quality examples\n",
        "- **Domain adaptation**: Selecting text that matches a specific domain or style\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
