{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# dPrune: K-Means Clustering-Based Pruning Example\n",
        "\n",
        "This notebook demonstrates how to use the **KMeansCentroidDistanceScorer** in `dPrune`. This is an unsupervised pruning method that scores examples based on their distance to cluster centroids in embedding space.\n",
        "\n",
        "## Key Concepts:\n",
        "- **Unsupervised**: No labels required for scoring\n",
        "- **Embedding-based**: Uses transformer model embeddings (CLS tokens)\n",
        "- **Clustering**: Groups similar examples and measures distances to centroids\n",
        "- **Distance scoring**: Examples closer to centroids get lower scores (more representative)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed\n",
        "# !pip install -e .[test]\n",
        "# !pip install transformers datasets torch scikit-learn tqdm matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# dPrune imports\n",
        "from dprune.scorers.unsupervised import KMeansCentroidDistanceScorer, get_cls_embeddings\n",
        "from dprune.pruners.selection import TopKPruner, BottomKPruner, StratifiedPruner\n",
        "from dprune.pipeline import PruningPipeline\n",
        "\n",
        "print(\"All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Create a Diverse Dataset\n",
        "\n",
        "We'll create a dataset with different types of text to demonstrate how clustering can identify diverse content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the TREC dataset from Hugging Face\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "# Load the dataset\n",
        "trec_dataset = load_dataset(\"CogComp/trec\", split=\"train\")\n",
        "\n",
        "# Get the label names for coarse categories\n",
        "coarse_label_names = trec_dataset.features['coarse_label'].names\n",
        "print(f\"Coarse label categories: {coarse_label_names}\")\n",
        "\n",
        "# Add readable category names to the dataset\n",
        "def add_category_name(example):\n",
        "    example['category'] = coarse_label_names[example['coarse_label']]\n",
        "    return example\n",
        "\n",
        "raw_dataset = trec_dataset.map(add_category_name)\n",
        "\n",
        "# Sample a subset for demonstration (use first 200 examples for faster processing)\n",
        "# In practice, you can use the full dataset\n",
        "sample_size = 200\n",
        "indices = list(range(len(raw_dataset)))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "sample_indices = indices[:sample_size]\n",
        "\n",
        "# Create a smaller dataset for this example\n",
        "raw_dataset = raw_dataset.select(sample_indices)\n",
        "\n",
        "print(f\"Dataset loaded with {len(raw_dataset)} examples (sampled from {len(trec_dataset)} total)\")\n",
        "print(f\"Categories: {set(raw_dataset['category'])}\")\n",
        "\n",
        "# Count examples per category\n",
        "category_counts = {}\n",
        "for cat in raw_dataset['category']:\n",
        "    category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "print(f\"\\nExamples per category:\")\n",
        "for category, count in category_counts.items():\n",
        "    print(f\"  {category}: {count}\")\n",
        "\n",
        "print(\"\\nSample texts from each category:\")\n",
        "seen_categories = set()\n",
        "for i, example in enumerate(raw_dataset):\n",
        "    if example['category'] not in seen_categories:\n",
        "        print(f\"{example['category']}: '{example['text']}'\")\n",
        "        seen_categories.add(example['category'])\n",
        "        if len(seen_categories) >= 6:  # Show all 6 categories\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Setup Model and Extract Embeddings\n",
        "\n",
        "We'll use a pre-trained transformer model to extract embeddings for our text data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a pre-trained model for embeddings\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Loaded model: {model_name}\")\n",
        "\n",
        "# Extract embeddings using the helper function\n",
        "embeddings = get_cls_embeddings(raw_dataset, model, tokenizer, text_column='text')\n",
        "\n",
        "print(f\"Extracted embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
        "\n",
        "# Add embeddings to dataset\n",
        "dataset_with_embeddings = raw_dataset.add_column('embedding', embeddings.tolist())\n",
        "print(f\"Dataset now has columns: {dataset_with_embeddings.column_names}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Visualize Embeddings with PCA\n",
        "\n",
        "Let's visualize the embeddings in 2D space to see how well they cluster by content type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce dimensionality for visualization\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Create a color map for categories\n",
        "category_colors = {'Technology': 'red', 'Sports': 'blue', 'Cooking': 'green', 'Science': 'orange'}\n",
        "colors = [category_colors[cat] for cat in categories]\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors, alpha=0.7, s=100)\n",
        "\n",
        "# Add labels for each point\n",
        "for i, (x, y) in enumerate(embeddings_2d):\n",
        "    plt.annotate(f\"{i}\", (x, y), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "# Create legend\n",
        "for category, color in category_colors.items():\n",
        "    plt.scatter([], [], c=color, label=category, s=100)\n",
        "\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "plt.title('Text Embeddings Visualization (PCA)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"PCA explains {pca.explained_variance_ratio_.sum():.1%} of the total variance\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Apply K-Means Clustering Scorer\n",
        "\n",
        "Now we'll use the `KMeansCentroidDistanceScorer` to score our examples based on their distance to cluster centroids.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the K-means scorer\n",
        "# We'll use 4 clusters since we have 4 content categories\n",
        "kmeans_scorer = KMeansCentroidDistanceScorer(num_clusters=4, random_state=42)\n",
        "\n",
        "# Score the dataset\n",
        "scored_dataset = kmeans_scorer.score(dataset_with_embeddings)\n",
        "\n",
        "print(\"Dataset scored with K-means centroid distances!\")\n",
        "print(f\"Scored dataset columns: {scored_dataset.column_names}\")\n",
        "\n",
        "# Examine the scores\n",
        "scores = scored_dataset['score']\n",
        "print(f\"\\nScore statistics:\")\n",
        "print(f\"  Min score: {min(scores):.3f}\")\n",
        "print(f\"  Max score: {max(scores):.3f}\")\n",
        "print(f\"  Mean score: {np.mean(scores):.3f}\")\n",
        "print(f\"  Std score: {np.std(scores):.3f}\")\n",
        "\n",
        "print(\"\\nFirst few examples with scores:\")\n",
        "for i in range(5):\n",
        "    print(f\"  Score: {scores[i]:.3f}, Category: {scored_dataset['category'][i]}, Text: '{scored_dataset['text'][i][:60]}...'\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Visualize Clusters and Scores\n",
        "\n",
        "Let's visualize the clusters found by K-means and see how the scores relate to the clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cluster assignments from the scorer\n",
        "cluster_labels = kmeans_scorer.kmeans.labels_\n",
        "centroids = kmeans_scorer.kmeans.cluster_centers_\n",
        "\n",
        "# Project centroids to 2D for visualization\n",
        "centroids_2d = pca.transform(centroids)\n",
        "\n",
        "# Create subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Plot 1: Clusters found by K-means\n",
        "scatter1 = ax1.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7, s=100)\n",
        "ax1.scatter(centroids_2d[:, 0], centroids_2d[:, 1], c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "ax1.set_title('K-Means Clusters')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add cluster labels\n",
        "for i, (x, y) in enumerate(embeddings_2d):\n",
        "    ax1.annotate(f\"{i}({cluster_labels[i]})\", (x, y), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "# Plot 2: Scores as colors\n",
        "scatter2 = ax2.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=scores, cmap='coolwarm', alpha=0.7, s=100)\n",
        "ax2.scatter(centroids_2d[:, 0], centroids_2d[:, 1], c='black', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "ax2.set_title('Distance Scores (Blue=Low, Red=High)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add score labels\n",
        "for i, (x, y) in enumerate(embeddings_2d):\n",
        "    ax2.annotate(f\"{i}({scores[i]:.2f})\", (x, y), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "# Add colorbars\n",
        "plt.colorbar(scatter1, ax=ax1, label='Cluster ID')\n",
        "plt.colorbar(scatter2, ax=ax2, label='Distance Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze cluster composition\n",
        "print(\"Cluster composition by category:\")\n",
        "for cluster_id in range(4):\n",
        "    cluster_mask = cluster_labels == cluster_id\n",
        "    cluster_categories = [categories[i] for i in range(len(categories)) if cluster_mask[i]]\n",
        "    category_counts = {cat: cluster_categories.count(cat) for cat in set(cluster_categories)}\n",
        "    print(f\"  Cluster {cluster_id}: {category_counts}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Different Pruning Strategies\n",
        "\n",
        "Let's explore different pruning strategies using the clustering-based scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 1: Keep examples closest to centroids (most representative)\n",
        "bottom_pruner = BottomKPruner(k=0.5)  # Keep bottom 50% (lowest distances)\n",
        "pipeline_representative = PruningPipeline(scorer=kmeans_scorer, pruner=bottom_pruner)\n",
        "representative_examples = pipeline_representative.run(dataset_with_embeddings)\n",
        "\n",
        "# Strategy 2: Keep examples farthest from centroids (most diverse/outliers)\n",
        "top_pruner = TopKPruner(k=0.5)  # Keep top 50% (highest distances)\n",
        "pipeline_diverse = PruningPipeline(scorer=kmeans_scorer, pruner=top_pruner)\n",
        "diverse_examples = pipeline_diverse.run(dataset_with_embeddings)\n",
        "\n",
        "# Strategy 3: Stratified sampling across score ranges\n",
        "stratified_pruner = StratifiedPruner(k=0.5, num_strata=4)\n",
        "pipeline_stratified = PruningPipeline(scorer=kmeans_scorer, pruner=stratified_pruner)\n",
        "stratified_examples = pipeline_stratified.run(dataset_with_embeddings)\n",
        "\n",
        "print(\"Pruning Results:\")\n",
        "print(f\"Original dataset: {len(scored_dataset)} examples\")\n",
        "print(f\"Representative examples (closest to centroids): {len(representative_examples)} examples\")\n",
        "print(f\"Diverse examples (farthest from centroids): {len(diverse_examples)} examples\")\n",
        "print(f\"Stratified examples (balanced across score ranges): {len(stratified_examples)} examples\")\n",
        "\n",
        "def analyze_category_distribution(dataset, name):\n",
        "    category_counts = {}\n",
        "    for cat in dataset['category']:\n",
        "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "    print(f\"\\n{name} category distribution:\")\n",
        "    for cat, count in category_counts.items():\n",
        "        percentage = (count / len(dataset)) * 100\n",
        "        print(f\"  {cat}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "analyze_category_distribution(representative_examples, \"Representative examples\")\n",
        "analyze_category_distribution(diverse_examples, \"Diverse examples\")\n",
        "analyze_category_distribution(stratified_examples, \"Stratified examples\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Detailed Analysis of Selected Examples\n",
        "\n",
        "Let's examine specific examples from each pruning strategy to understand what types of content they select.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== REPRESENTATIVE EXAMPLES (Closest to Centroids) ===\")\n",
        "print(\"These are the most 'typical' examples of each cluster:\")\n",
        "rep_scores = representative_examples['score']\n",
        "rep_indices = [scored_dataset['text'].index(text) for text in representative_examples['text']]\n",
        "\n",
        "for i, (idx, score) in enumerate(zip(rep_indices, rep_scores)):\n",
        "    print(f\"\\nExample {i+1} (Original index: {idx}, Score: {score:.3f}):\")\n",
        "    print(f\"  Category: {representative_examples['category'][i]}\")\n",
        "    print(f\"  Cluster: {cluster_labels[idx]}\")\n",
        "    print(f\"  Text: '{representative_examples['text'][i]}'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=== DIVERSE EXAMPLES (Farthest from Centroids) ===\")\n",
        "print(\"These are the most 'unusual' or outlier examples:\")\n",
        "div_scores = diverse_examples['score']\n",
        "div_indices = [scored_dataset['text'].index(text) for text in diverse_examples['text']]\n",
        "\n",
        "for i, (idx, score) in enumerate(zip(div_indices, div_scores)):\n",
        "    print(f\"\\nExample {i+1} (Original index: {idx}, Score: {score:.3f}):\")\n",
        "    print(f\"  Category: {diverse_examples['category'][i]}\")\n",
        "    print(f\"  Cluster: {cluster_labels[idx]}\")\n",
        "    print(f\"  Text: '{diverse_examples['text'][i]}'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=== SCORE DISTRIBUTION ANALYSIS ===\")\n",
        "\n",
        "# Plot score distributions for different strategies\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Original scores\n",
        "axes[0,0].hist(scored_dataset['score'], bins=10, alpha=0.7, color='gray', edgecolor='black')\n",
        "axes[0,0].set_title('Original Dataset Scores')\n",
        "axes[0,0].set_xlabel('Distance Score')\n",
        "axes[0,0].set_ylabel('Frequency')\n",
        "\n",
        "# Representative examples scores\n",
        "axes[0,1].hist(representative_examples['score'], bins=10, alpha=0.7, color='blue', edgecolor='black')\n",
        "axes[0,1].set_title('Representative Examples Scores')\n",
        "axes[0,1].set_xlabel('Distance Score')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "\n",
        "# Diverse examples scores\n",
        "axes[1,0].hist(diverse_examples['score'], bins=10, alpha=0.7, color='red', edgecolor='black')\n",
        "axes[1,0].set_title('Diverse Examples Scores')\n",
        "axes[1,0].set_xlabel('Distance Score')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "\n",
        "# Stratified examples scores\n",
        "axes[1,1].hist(stratified_examples['score'], bins=10, alpha=0.7, color='green', edgecolor='black')\n",
        "axes[1,1].set_title('Stratified Examples Scores')\n",
        "axes[1,1].set_xlabel('Distance Score')\n",
        "axes[1,1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Comparing Different Numbers of Clusters\n",
        "\n",
        "Let's experiment with different numbers of clusters to see how it affects the scoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different numbers of clusters\n",
        "cluster_numbers = [2, 3, 4, 6, 8]\n",
        "fig, axes = plt.subplots(1, len(cluster_numbers), figsize=(20, 4))\n",
        "\n",
        "for i, n_clusters in enumerate(cluster_numbers):\n",
        "    # Create scorer with different number of clusters\n",
        "    scorer = KMeansCentroidDistanceScorer(num_clusters=n_clusters, random_state=42)\n",
        "    temp_scored = scorer.score(dataset_with_embeddings)\n",
        "    temp_labels = scorer.kmeans.labels_\n",
        "    \n",
        "    # Plot the clustering\n",
        "    scatter = axes[i].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
        "                             c=temp_labels, cmap='tab10', alpha=0.7, s=50)\n",
        "    \n",
        "    # Plot centroids\n",
        "    temp_centroids_2d = pca.transform(scorer.kmeans.cluster_centers_)\n",
        "    axes[i].scatter(temp_centroids_2d[:, 0], temp_centroids_2d[:, 1], \n",
        "                   c='red', marker='x', s=100, linewidths=2)\n",
        "    \n",
        "    axes[i].set_title(f'{n_clusters} Clusters')\n",
        "    axes[i].set_xlabel('PC1')\n",
        "    axes[i].set_ylabel('PC2')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Calculate and print inertia (within-cluster sum of squares)\n",
        "    inertia = scorer.kmeans.inertia_\n",
        "    axes[i].text(0.02, 0.98, f'Inertia: {inertia:.1f}', \n",
        "                transform=axes[i].transAxes, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze how scores change with different cluster numbers\n",
        "print(\"Score statistics for different numbers of clusters:\")\n",
        "print(\"Clusters | Mean Score | Std Score | Min Score | Max Score\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for n_clusters in cluster_numbers:\n",
        "    scorer = KMeansCentroidDistanceScorer(num_clusters=n_clusters, random_state=42)\n",
        "    temp_scored = scorer.score(dataset_with_embeddings)\n",
        "    scores = temp_scored['score']\n",
        "    \n",
        "    print(f\"{n_clusters:8d} | {np.mean(scores):10.3f} | {np.std(scores):9.3f} | {min(scores):9.3f} | {max(scores):9.3f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. Conclusion and Key Insights\n",
        "\n",
        "This notebook demonstrated the power of unsupervised clustering-based pruning with the `KMeansCentroidDistanceScorer`.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Key Insights from this Example:\n",
        "\n",
        "1. **Unsupervised Nature**: The `KMeansCentroidDistanceScorer` doesn't require labels, making it useful for unlabeled datasets or when you want to discover natural groupings in your data.\n",
        "\n",
        "2. **Embedding Quality Matters**: The quality of the clustering depends heavily on the quality of the embeddings. Pre-trained transformer models like DistilBERT provide good semantic representations.\n",
        "\n",
        "3. **Pruning Strategies**:\n",
        "   - **Representative Examples** (low scores): Keep examples closest to cluster centroids for core/typical content\n",
        "   - **Diverse Examples** (high scores): Keep outliers and edge cases for challenging/diverse content\n",
        "   - **Stratified Sampling**: Maintain balanced representation across the score spectrum\n",
        "\n",
        "4. **Cluster Number Selection**: \n",
        "   - Too few clusters: May miss important distinctions in your data\n",
        "   - Too many clusters: May create overly specific groups\n",
        "   - Consider your domain knowledge and data characteristics\n",
        "\n",
        "5. **Interpretability**: The clustering often aligns with semantic categories, making the pruning decisions interpretable and explainable.\n",
        "\n",
        "6. **Scalability**: This method scales well to larger datasets since it only requires embedding extraction and K-means clustering.\n",
        "\n",
        "### When to Use K-Means Clustering Pruning:\n",
        "\n",
        "- ✅ **Unlabeled datasets** where you can't use supervised methods\n",
        "- ✅ **Diversity preservation** when you want to maintain data variety\n",
        "- ✅ **Content discovery** to understand natural groupings in your data\n",
        "- ✅ **Balanced sampling** across different content types\n",
        "- ✅ **Large datasets** where computational efficiency matters\n",
        "\n",
        "### Practical Applications:\n",
        "\n",
        "- **Dataset curation** for training more focused models\n",
        "- **Content recommendation** by finding representative examples\n",
        "- **Data exploration** to understand dataset composition\n",
        "- **Quality control** by identifying outliers or unusual examples\n",
        "- **Efficient annotation** by selecting diverse examples for labeling\n",
        "\n",
        "The `KMeansCentroidDistanceScorer` provides a powerful, interpretable, and scalable approach to unsupervised data pruning that can significantly improve your machine learning workflows!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
